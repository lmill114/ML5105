# -*- coding: utf-8 -*-
"""LMHW1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UuXVewyOTs-TKLfdEtahfbn7WTGIAO3U
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Import file from Google Drive
from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/My Drive/D3.csv')

print(df.head())

X = df.values[:, 0]  # get input values from first column -- X is a list here
X2 = df.values[:, 1] # get values for X2
X3 = df.values[:, 2] # get values for X3
y = df.values[:, 3]  # get output values from second column -- Y is the list here
m = len(y)  # Number of training examples
n = len(X)  # Number of training examples
n2 = len(X2)  # Number of training examples
n3 = len(X3)  # Number of training examples

print('X = ', X[: 10])
print('y = ', y[: 10])
print('m = ', m)
print('n = ', n)
print('n2 = ', n2)
print('n3 = ', n3)

"""Plot training data."""

from IPython.display import display
display(df)

X = df.values[:, 0]  # get input values from the first column -- X is a list here which is a 1 dimensional array
X2 = df.values[:, 1] # get input values from the second column -- X2 is a list here which is a 1 dimensional array
X3 = df.values[:, 2] # get input values from the third column -- X3 is a list here which is a 1 dimensional array
y = df.values[:, 3]  # get output values from the fourth column --  Y is a list here which is a 1 dimensional array

# Scatter plot
plt.scatter(X, y, color='red', marker='+')

# Grid, labels, and title
plt.grid(True)
plt.rcParams["figure.figsize"] = (6, 6)
plt.xlabel('X1')
plt.ylabel('Y')
plt.title('Scatter plot of training data X1')

# Show the plot
plt.show()

# Scatter plot 2
plt.scatter(X2, y, color='green', marker='+')

# Grid, labels, and title 2
plt.grid(True)
plt.rcParams["figure.figsize"] = (6, 6)
plt.xlabel('X2')
plt.ylabel('Y')
plt.title('Scatter plot of training data X2')

# Show the plot
plt.show()

# Scatter plot 3
plt.scatter(X3, y, color='blue', marker='+')

# Grid, labels, and title 3
plt.grid(True)
plt.rcParams["figure.figsize"] = (6, 6)
plt.xlabel('X3')
plt.ylabel('Y')
plt.title('Scatter plot of training data X3')

# Show the plot
plt.show()

X_0 = np.ones((m, 1)) # Create an matrix of m ones for matrix operations

# Reshape datasets for matrix operations
X_1 = X.reshape(m, 1)
X_2 = X2.reshape(m, 1)
X_3 = X3.reshape(m, 1)

# Stack Matrices
X = np.hstack((X_0, X_1))
X2 = np.hstack((X_0, X_2))
X3 = np.hstack((X_0, X_3))

# Set theta to 0
theta = np.zeros(2)
theta2 = np.zeros(2)
theta3 = np.zeros(2)

theta
theta2
theta3

def compute_cost(X, y, theta):

  predictions = X.dot(theta)
  errors = np.subtract(predictions, y)
  sqrErrors = np.square(errors)
  J = 1 / (2 * m) * np.sum(sqrErrors)
  return J

# Print Cost of Each Set at theta = zeros
cost = compute_cost(X, y, theta)
cost2 = compute_cost(X2, y, theta2)
cost3 = compute_cost(X3, y, theta3)
print('The cost for given X values of theta_0 and theta_1 =', cost)
print('The cost for given X2 values of theta_0 and theta_2 =', cost2)
print('The cost for given X3 values of theta_0 and theta_2 =', cost3)

def gradient_descent(X, y, theta, alpha, iterations):
  m = len(y)  # Number of training examples
  cost_history = np.zeros(iterations)

  for i in range(iterations):
    predictions = X.dot(theta)
    errors = np.subtract(predictions, y)
    sum_delta = (alpha / m) * X.transpose().dot(errors)
    theta -= sum_delta
    cost_history[i] = compute_cost(X, y, theta)

  return theta, cost_history

# Set variables
theta = [0., 0.]
iterations = 1500
alpha = 0.1

# Determine theta and cost for X1
theta, cost_history = gradient_descent(X, y, theta, alpha, iterations)
print('Final value of theta =', theta)
print('cost_history =', cost_history)

# Determine theta and cost for X2
theta2, cost_history2 = gradient_descent(X2, y, theta2, alpha, iterations)
print('Final value of theta2 =', theta2)
print('cost_history2 =', cost_history2)

# Determine theta and cost for X3
theta3, cost_history3 = gradient_descent(X3, y, theta3, alpha, iterations)
print('Final value of theta3 =', theta3)
print('cost_history3 =', cost_history3)

#Plot for X1
plt.scatter(X[:, 1], y, color='red', marker='+', label='Training Data')

# Line plot for the linear regression model
plt.plot(X[:, 1], X.dot(theta), color='green', label='Linear Regression')

# Plot customizations
plt.rcParams["figure.figsize"] = (10, 6)
plt.grid(True)
plt.xlabel('X1')
plt.ylabel('Y')
plt.title('Linear Regression Fit X1')
plt.legend()

# Show the plot
plt.show()

# Display equation of the first linear regression model
x1Equation = f"Regression Line:   y = {theta[1]}x + {theta[0]}"
print(x1Equation)

#Plot for X2
plt.scatter(X2[:, 1], y, color='green', marker='+', label='Training Data')

# Line plot for the linear regression model 2
plt.plot(X2[:, 1], X2.dot(theta2), color='red', label='Linear Regression')

# Plot customizations 2
plt.rcParams["figure.figsize"] = (10, 6)
plt.grid(True)
plt.xlabel('X2')
plt.ylabel('Y')
plt.title('Linear Regression Fit X2')
plt.legend()

# Show the plot 2
plt.show()

# Display equation of the second linear regression model
x2Equation = f"Regression Line:   y = {theta2[1]}x + {theta2[0]}"
print(x2Equation)

#Plot for X3
plt.scatter(X3[:, 1], y, color='blue', marker='+', label='Training Data')

# Line plot for the linear regression model 3
plt.plot(X3[:, 1], X3.dot(theta3), color='red', label='Linear Regression')

# Plot customizations 3
plt.rcParams["figure.figsize"] = (10, 6)
plt.grid(True)
plt.xlabel('X3')
plt.ylabel('Y')
plt.title('Linear Regression Fit X3')
plt.legend()

# Show the plot 3
plt.show()

# Display equation of the third linear regression model
x3Equation = f"Regression Line:   y = {theta3[1]}x + {theta3[0]}"
print(x3Equation)

#Plot for X1
plt.plot(range(1, iterations + 1), cost_history, color='red')
plt.rcParams["figure.figsize"] = (10, 6)
plt.grid(True)

plt.xlabel('Number of iterations')
plt.ylabel('Cost (J)')
plt.title('Convergence of Gradient Descent X1')

# Show the plot
plt.show()

# Print Cost History for X1
print(cost_history)

#Plot for X2
plt.plot(range(1, iterations + 1), cost_history2, color='green')
plt.rcParams["figure.figsize"] = (10, 6)
plt.grid(True)

plt.xlabel('Number of iterations')
plt.ylabel('Cost (J)')
plt.title('Convergence of Gradient Descent X2')

# Show the plot
plt.show()

# Print Cost History for X1
print(cost_history2)

#Plot for X3
plt.plot(range(1, iterations + 1), cost_history3, color='blue')
plt.rcParams["figure.figsize"] = (10, 6)
plt.grid(True)

plt.xlabel('Number of iterations')
plt.ylabel('Cost (J)')
plt.title('Convergence of Gradient Descent X3')

# Show the plot
plt.show()

# Print Cost History for X1
print(cost_history3)

"""Problem 2: Instead of 3 1-Dimensional linear regressions make 1 regression line using 3-Dimensions


"""

#Combine all three columns from D3.csv into one matrix (with addional X_0 defined previously)
X_all = np.hstack((X_0, X_1, X_2, X_3))

#Create new theta to match dimensions of X_all
theta_all = np.zeros(4)

#Update alpha
alpha = 0.1

#Run gradient descent for new model
theta_all, cost_history_all = gradient_descent(X_all, y, theta_all, alpha, iterations)

#Plot for X_all
plt.plot(range(1, iterations + 1), cost_history_all, color='blue')
plt.rcParams["figure.figsize"] = (10, 6)
plt.grid(True)

plt.xlabel('Number of iterations')
plt.ylabel('Cost (J)')
plt.title('Convergence of Gradient Descent X all')

# Show the plot
plt.show()

print(cost_history_all)

"""
As the learning rate approaches 0.1, the loss reaches the minimum after less iterations.


"""

# New values for (X1, X2, X3)
test1 = np.array([1, 1, 1])  # (1, 1, 1)
test2 = np.array([2, 0, 4])  # (2, 0, 4)
test3 = np.array([3, 2, 1])  # (3, 2, 1)

# Create a matrix of ones for the bias term
X_0 = np.ones((1, 1))  # Shape (1, 1) since we're predicting for one example at a time

# Prepare the input matrices by adding the bias term (X_0)
Xp1 = np.hstack((X_0, test1.reshape(1, 3)))  # Shape (1, 4)
Xp2 = np.hstack((X_0, test2.reshape(1, 3)))  # Shape (1, 4)
Xp3 = np.hstack((X_0, test3.reshape(1, 3)))  # Shape (1, 4)

# Predict y for the new values using the learned theta_all
prediction1 = Xp1.dot(theta_all)
prediction2 = Xp2.dot(theta_all)
prediction3 = Xp3.dot(theta_all)

# Print the linear model with the trained theta values
print(f"Linear Regression Model: y = {theta_all[0]} + {theta_all[1]}(X1) + {theta_all[2]}(X2) + {theta_all[3]}(X3)")

# Print the predictions
print(f"Prediction for (1, 1, 1): {prediction1[0]}")
print(f"Prediction for (2, 0, 4): {prediction2[0]}")
print(f"Prediction for (3, 2, 1): {prediction3[0]}")